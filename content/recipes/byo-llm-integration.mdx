---
title: Custom LLM
description: Use your own language model while Anam handles speech-to-text, text-to-speech, and avatar rendering.
tags: [custom-llm, javascript, intermediate]
difficulty: intermediate
sdk: javascript
date: 2025-01-18
author: Anam Team
---

# Custom LLM

This recipe shows how to use your own language model with Anam. Anam handles speech-to-text, text-to-speech, and avatar rendering while you handle the conversation logic.

## How it works

In BYO LLM mode:
1. Anam transcribes user speech and sends you the text
2. You process it with your LLM and send back a response
3. Anam speaks the response through the avatar

You handle:
- Conversation logic and context
- RAG/retrieval
- Tool calling
- Response filtering

## Prerequisites

- Anam account with API key
- Your LLM endpoint (OpenAI, Anthropic, self-hosted, etc.)
- Node.js 18+

## Step 1: Create a BYO LLM Persona

First, create a persona configured for BYO LLM mode in Anam Lab, or via the API:

```typescript
const persona = await fetch('https://api.anam.ai/v1/personas', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${process.env.ANAM_API_KEY}`,
  },
  body: JSON.stringify({
    name: 'My BYO LLM Agent',
    avatarId: 'your-avatar-id',
    voiceId: 'your-voice-id',
    llmId: 'CUSTOMER_CLIENT_V1', // This disables the built-in LLM and enables BYO mode
  }),
});
```

<Info>
Setting `llmId: 'CUSTOMER_CLIENT_V1'` disables the built-in LLM so you can provide your own responses.
</Info>

## Step 2: Set Up the Message Handler

Configure your client to receive transcriptions and send responses:

```typescript
import { createClient, AnamEvent } from '@anam-ai/js-sdk';
import OpenAI from 'openai';

const openai = new OpenAI();
const conversationHistory: Array<{ role: string; content: string }> = [];

const client = createClient(sessionToken);

// Listen for user speech transcriptions
client.addListener(AnamEvent.MESSAGE_STREAM_EVENT_RECEIVED, async (event) => {
  // Only process final user messages
  if (event.role !== 'user' || !event.endOfSpeech) return;

  console.log('User said:', event.content);

  // Add to conversation history
  conversationHistory.push({
    role: 'user',
    content: event.content,
  });

  // Get response from your LLM
  const response = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      ...conversationHistory,
    ],
  });

  const assistantMessage = response.choices[0].message.content;

  // Add to history
  conversationHistory.push({
    role: 'assistant',
    content: assistantMessage,
  });

  // Send to Anam for avatar to speak
  await client.talk(assistantMessage);
});
```

## Step 3: Stream responses

Streaming reduces latency because the avatar starts speaking before the full response is ready:

```typescript
client.addListener(AnamEvent.MESSAGE_STREAM_EVENT_RECEIVED, async (event) => {
  if (event.role !== 'user' || !event.endOfSpeech) return;

  conversationHistory.push({ role: 'user', content: event.content });

  const llmStream = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      ...conversationHistory,
    ],
    stream: true,
  });

  // Create a talk stream to send chunks to the avatar
  const talkStream = client.createTalkMessageStream();
  let fullResponse = '';

  for await (const chunk of llmStream) {
    const content = chunk.choices[0]?.delta?.content || '';
    fullResponse += content;

    // Stream chunks to Anam as they arrive
    if (content) {
      await talkStream.streamMessageChunk(content, false);
    }
  }

  // Signal end of message
  await talkStream.endMessage();

  conversationHistory.push({ role: 'assistant', content: fullResponse });
});
```


## Step 4: Add Tool Calling

You can implement tool calling entirely in your LLM logic:

```typescript
const tools = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get current weather for a location',
      parameters: {
        type: 'object',
        properties: {
          location: { type: 'string', description: 'City name' },
        },
        required: ['location'],
      },
    },
  },
];

client.addListener(AnamEvent.MESSAGE_STREAM_EVENT_RECEIVED, async (event) => {
  if (event.role !== 'user' || !event.endOfSpeech) return;

  conversationHistory.push({ role: 'user', content: event.content });

  const response = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: conversationHistory,
    tools,
  });

  const message = response.choices[0].message;

  if (message.tool_calls) {
    // Execute tool calls
    for (const toolCall of message.tool_calls) {
      if (toolCall.function.name === 'get_weather') {
        const args = JSON.parse(toolCall.function.arguments);
        const weather = await getWeather(args.location);

        // Add tool result to conversation
        conversationHistory.push({
          role: 'tool',
          content: JSON.stringify(weather),
          tool_call_id: toolCall.id,
        });
      }
    }

    // Get final response with tool results
    const finalResponse = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: conversationHistory,
    });

    await client.talk(finalResponse.choices[0].message.content);
  } else {
    await client.talk(message.content);
  }
});
```

## Complete Example

```typescript
import { createClient, AnamEvent } from '@anam-ai/js-sdk';
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic();

async function startBYOLLMSession() {
  const client = createClient(await getSessionToken());

  const history: Array<{ role: 'user' | 'assistant'; content: string }> = [];

  client.addListener(AnamEvent.MESSAGE_STREAM_EVENT_RECEIVED, async (event) => {
    // Only process final user transcripts
    if (event.role !== 'user' || !event.endOfSpeech) return;

    history.push({ role: 'user', content: event.content });

    // Use Claude for responses
    const response = await anthropic.messages.create({
      model: 'claude-sonnet-4-20250514',
      max_tokens: 1024,
      system: 'You are a friendly customer support agent for an e-commerce site.',
      messages: history,
    });

    const assistantMessage = response.content[0].text;
    history.push({ role: 'assistant', content: assistantMessage });

    await client.talk(assistantMessage);
  });

  await client.streamToVideoElement('avatar');
}
```

## Next Steps

- [Add RAG to your LLM](/rag-knowledge-base) - Ground responses in your documentation
- [Implement guardrails](/llm-guardrails) - Add safety filters to responses
- [Handle interruptions](/handling-interruptions) - Cancel in-flight responses when users interrupt
